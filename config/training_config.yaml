model:
  name: "meta-llama/Llama-3.2-3B-Instruct"
  quantization:
    enabled: true  # Set to true for AWS
    load_in_4bit: true
    compute_dtype: "float16"
    quant_type: "nf4"
    use_double_quant: true

training:
  learning_rate: 2e-4
  batch_size: 4
  num_epochs: 4
  max_length: 256
  gradient_accumulation_steps: 4
  warmup_steps: 100
  weight_decay: 0.01

lora:
  r: 8
  alpha: 32
  dropout: 0.1
  target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]
  bias: "none"
  task_type: "CAUSAL_LM"